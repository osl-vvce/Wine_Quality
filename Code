import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns; sns.set(style="ticks", color_codes=True)


winepath='winequality-red.csv'  #file path
winedata=pd.read_csv(winepath) #read data
X=winedata.iloc[:,1:].values

# Using elbow method to find the optimal number of clusters.
from sklearn.cluster import KMeans
wcss=[]
for i in range(1,11):
    kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,11),wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

kmeans=KMeans(n_clusters=2,init='k-means++',max_iter=300,n_init=10,random_state=0)    
y_kmeans=kmeans.fit_predict(X)
plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],c='red',label='1')
plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],c='blue',label='2')
plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],c='green',label='3')
plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=50,c='yellow',label='Centroids')
plt.title('Clusters of Wines')
plt.xlabel('Quality')
plt.ylabel('Features')
plt.legend()
plt.show()

#sns.pairplot(winedata,
#             x_vars=['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','pH'],
#             y_vars=["quality"], kind="reg",markers="+")
y=winedata.quality              #setting the prediction feature
winefeatures=['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide','density','pH','sulphates','alcohol']    #features to predict y
x=winedata[winefeatures]
winemodel=DecisionTreeRegressor(random_state=1)   #using decision tree for prediction
winemodel.fit(x,y)
#print("Making predictions for the following 5 wines:")
#print(x.tail())
print("The predictions are")
print(winemodel.predict(x.iloc[:,:]))
winepred = winemodel.predict(x)                   #predicting for the first 5 values
mean_absolute_error(y, winepred)                  #calculate the error
train_x, val_x, train_y, val_y = train_test_split(x, y, random_state = 0)
winemodel = DecisionTreeRegressor()
winemodel.fit(train_x, train_y)
valpred = winemodel.predict(val_x)
print(mean_absolute_error(val_y, valpred))

